{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Water Conflict Classifier - Demo Notebook\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Loading the public dataset from Hugging Face\n",
        "2. Loading the pre-trained model\n",
        "3. Training (or fine-tuning) the model\n",
        "4. Testing the model\n",
        "5. Making predictions on custom headlines\n",
        "\n",
        "**Dataset**: `baobabtech/water-conflict-training-data`  \n",
        "**Model**: `baobabtech/water-conflict-classifier`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q setfit datasets sentence-transformers scikit-learn pandas numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset from Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from setfit import SetFitModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
        "\n",
        "# Load dataset from HF Hub\n",
        "print(\"Loading dataset from Hugging Face Hub...\")\n",
        "dataset = load_dataset(\"baobabtech/water-conflict-training-data\")\n",
        "\n",
        "# Combine train split into a DataFrame\n",
        "df = dataset['train'].to_pandas()\n",
        "print(f\"\\nLoaded {len(df)} examples\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "Convert the raw data into multi-label format for SetFit training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label names\n",
        "LABEL_NAMES = ['Trigger', 'Casualty', 'Weapon']\n",
        "\n",
        "def preprocess_for_training(df):\n",
        "    \"\"\"\n",
        "    Convert dataset to multi-label format:\n",
        "    - text: headline\n",
        "    - labels: [Trigger, Casualty, Weapon] as [0/1, 0/1, 0/1]\n",
        "    \"\"\"\n",
        "    processed = df.copy()\n",
        "    processed['text'] = processed['Headline']\n",
        "    \n",
        "    # Parse labels from Basis column\n",
        "    processed['labels'] = processed['Basis'].apply(\n",
        "        lambda basis: [\n",
        "            1 if 'Trigger' in str(basis) else 0,\n",
        "            1 if 'Casualty' in str(basis) else 0,\n",
        "            1 if 'Weapon' in str(basis) else 0\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return processed[['text', 'labels']]\n",
        "\n",
        "# Preprocess\n",
        "data = preprocess_for_training(df)\n",
        "\n",
        "# Print label distribution\n",
        "label_counts = np.array(data['labels'].tolist()).sum(axis=0)\n",
        "print(\"\\nLabel distribution:\")\n",
        "for label_name, count in zip(LABEL_NAMES, label_counts):\n",
        "    print(f\"  - {label_name}: {int(count)} ({count/len(data)*100:.1f}%)\")\n",
        "\n",
        "# Count negatives (all zeros)\n",
        "neg_count = (data['labels'].apply(lambda x: x == [0, 0, 0])).sum()\n",
        "print(f\"  - Negatives (no labels): {neg_count} ({neg_count/len(data)*100:.1f}%)\")\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/test sets (80/20)\n",
        "train_data, test_data = train_test_split(\n",
        "    data, \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training examples: {len(train_data)}\")\n",
        "print(f\"Test examples: {len(test_data)}\")\n",
        "\n",
        "# Convert to lists for SetFit\n",
        "train_texts = train_data['text'].tolist()\n",
        "train_labels = train_data['labels'].tolist()\n",
        "test_texts = test_data['text'].tolist()\n",
        "test_labels = test_data['labels'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Option A: Load Pre-trained Model\n",
        "\n",
        "Load the already-trained model from Hugging Face Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "print(\"Loading pre-trained model from Hugging Face Hub...\")\n",
        "model = SetFitModel.from_pretrained(\"baobabtech/water-conflict-classifier\")\n",
        "print(\"✓ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Option B: Train a New Model (Alternative)\n",
        "\n",
        "Or train a new model from scratch using the dataset.\n",
        "\n",
        "**Note**: This will take significant time. Skip this if you just want to use the pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to train from scratch\n",
        "# from setfit import SetFitModel, Trainer, TrainingArguments\n",
        "# from datasets import Dataset\n",
        "\n",
        "# # Initialize model\n",
        "# model = SetFitModel.from_pretrained(\n",
        "#     \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
        "#     multi_target_strategy=\"one-vs-rest\",\n",
        "#     labels=LABEL_NAMES\n",
        "# )\n",
        "\n",
        "# # Create Dataset objects\n",
        "# train_dataset = Dataset.from_dict({\n",
        "#     \"text\": train_texts,\n",
        "#     \"label\": train_labels\n",
        "# })\n",
        "\n",
        "# test_dataset = Dataset.from_dict({\n",
        "#     \"text\": test_texts,\n",
        "#     \"label\": test_labels\n",
        "# })\n",
        "\n",
        "# # Training arguments\n",
        "# args = TrainingArguments(\n",
        "#     batch_size=16,\n",
        "#     num_epochs=1,\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     load_best_model_at_end=True,\n",
        "# )\n",
        "\n",
        "# # Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset,\n",
        "# )\n",
        "\n",
        "# # Train\n",
        "# print(\"Training model...\")\n",
        "# trainer.train()\n",
        "# print(\"✓ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_predictions = model.predict(test_texts)\n",
        "\n",
        "# Convert predictions to binary format\n",
        "if isinstance(test_predictions[0], list):\n",
        "    # Already in list format\n",
        "    y_pred = np.array(test_predictions)\n",
        "else:\n",
        "    # Convert from numpy array\n",
        "    y_pred = test_predictions\n",
        "\n",
        "y_true = np.array(test_labels)\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Overall metrics\n",
        "print(f\"\\nHamming Loss: {hamming_loss(y_true, y_pred):.4f}\")\n",
        "print(f\"Micro F1: {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
        "print(f\"Macro F1: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "print(f\"Weighted F1: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# Per-label metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PER-LABEL METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(\n",
        "    y_true, \n",
        "    y_pred, \n",
        "    target_names=LABEL_NAMES,\n",
        "    zero_division=0\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Try Out the Model on Custom Headlines\n",
        "\n",
        "Test the model on your own headlines!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_and_display(headlines):\n",
        "    \"\"\"\n",
        "    Predict labels for headlines and display results nicely.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(headlines)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PREDICTIONS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for headline, pred in zip(headlines, predictions):\n",
        "        labels = [LABEL_NAMES[i] for i, val in enumerate(pred) if val == 1]\n",
        "        \n",
        "        print(f\"\\nHeadline: {headline}\")\n",
        "        if labels:\n",
        "            print(f\"  Labels: {', '.join(labels)}\")\n",
        "        else:\n",
        "            print(f\"  Labels: None (not water conflict)\")\n",
        "        print(f\"  Raw: {pred}\")\n",
        "\n",
        "# Example headlines to test\n",
        "test_headlines = [\n",
        "    \"Armed groups attacked water treatment facility in northern region\",\n",
        "    \"Dispute over river water rights leads to protest\",\n",
        "    \"Government announces new education policy\",\n",
        "    \"Dam used as leverage in territorial conflict\",\n",
        "    \"Political party holds rally in capital city\",\n",
        "]\n",
        "\n",
        "predict_and_display(test_headlines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Prediction\n",
        "\n",
        "Enter your own headlines to classify:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive prediction\n",
        "while True:\n",
        "    headline = input(\"\\nEnter a headline to classify (or 'quit' to exit): \")\n",
        "    \n",
        "    if headline.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    \n",
        "    if not headline.strip():\n",
        "        continue\n",
        "    \n",
        "    predict_and_display([headline])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analyze Model Performance by Label\n",
        "\n",
        "Deep dive into which types of conflicts the model handles best.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze performance by label\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate per-label F1 scores\n",
        "f1_per_label = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(LABEL_NAMES, f1_per_label)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('Model Performance by Label Type')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(f1_per_label):\n",
        "    plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion info for each label\n",
        "print(\"\\nDetailed Analysis:\")\n",
        "for i, label_name in enumerate(LABEL_NAMES):\n",
        "    true_positives = ((y_true[:, i] == 1) & (y_pred[:, i] == 1)).sum()\n",
        "    false_positives = ((y_true[:, i] == 0) & (y_pred[:, i] == 1)).sum()\n",
        "    false_negatives = ((y_true[:, i] == 1) & (y_pred[:, i] == 0)).sum()\n",
        "    true_negatives = ((y_true[:, i] == 0) & (y_pred[:, i] == 0)).sum()\n",
        "    \n",
        "    print(f\"\\n{label_name}:\")\n",
        "    print(f\"  True Positives: {true_positives}\")\n",
        "    print(f\"  False Positives: {false_positives}\")\n",
        "    print(f\"  False Negatives: {false_negatives}\")\n",
        "    print(f\"  True Negatives: {true_negatives}\")\n",
        "    print(f\"  F1 Score: {f1_per_label[i]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Predictions\n",
        "\n",
        "Save test set predictions to CSV for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'headline': test_texts,\n",
        "    'true_trigger': y_true[:, 0],\n",
        "    'true_casualty': y_true[:, 1],\n",
        "    'true_weapon': y_true[:, 2],\n",
        "    'pred_trigger': y_pred[:, 0],\n",
        "    'pred_casualty': y_pred[:, 1],\n",
        "    'pred_weapon': y_pred[:, 2],\n",
        "})\n",
        "\n",
        "# Add correctness column\n",
        "results_df['all_correct'] = (\n",
        "    (results_df['true_trigger'] == results_df['pred_trigger']) &\n",
        "    (results_df['true_casualty'] == results_df['pred_casualty']) &\n",
        "    (results_df['true_weapon'] == results_df['pred_weapon'])\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "output_file = 'test_predictions.csv'\n",
        "results_df.to_csv(output_file, index=False)\n",
        "print(f\"\\n✓ Predictions saved to {output_file}\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nSample predictions:\")\n",
        "results_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "- ✓ Loading public dataset from HF Hub\n",
        "- ✓ Loading pre-trained model\n",
        "- ✓ Evaluating model performance\n",
        "- ✓ Making predictions on custom headlines\n",
        "- ✓ Analyzing per-label performance\n",
        "- ✓ Exporting results\n",
        "\n",
        "**Next Steps:**\n",
        "- Fine-tune the model on additional data\n",
        "- Experiment with different base models\n",
        "- Deploy as an API or web service\n",
        "- Apply to new datasets for conflict analysis\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
